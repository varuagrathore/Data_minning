{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRT19bWhFFch"
      },
      "source": [
        "**Data** **Preparation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9MFsUM4FFck"
      },
      "source": [
        "# Need of Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMaN8r1bFFck"
      },
      "source": [
        "# Tabular data Pre-processing  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6fCXAf_FFcl"
      },
      "source": [
        "- Structured data - Tabular data\n",
        "- Semistructured data - Json data\n",
        "- Unstructured data - Text file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXBnCMHVybJS"
      },
      "source": [
        "# Exploratory Data Analysis [EDA] on Titanic Dataset\n",
        "\n",
        "\n",
        "\n",
        ": Feature Engineering\n",
        "- Data Pre-processing and\n",
        "- **Exploratory Data Analysis [EDA]**\n",
        "- **Data Visualization**\n",
        "- Feature Extraction\n",
        "- Feature Selection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5gQ61apFFcl"
      },
      "source": [
        "**DATASET:** [https://www.kaggle.com/c/titanic/overview ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs8JFo86FFcm"
      },
      "source": [
        "## Data reading and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bg77bB9FFcm"
      },
      "outputs": [],
      "source": [
        "### Titanic Dataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBK9p2mvFFcn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "cd8435ee-3985-4d77-af8a-3e7b83383c41"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'test.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-63deb0cfec32>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.csv'"
          ]
        }
      ],
      "source": [
        "test_df = pd.read_csv(\"test.csv\")\n",
        "train_df = pd.read_csv(\"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpLeNFUHM52-",
        "outputId": "6d2449c8-9206-4179-9952-eb669819fa3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n"
          ]
        }
      ],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = train_df.isnull().sum()\n",
        "total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS1JhNlAy33r",
        "outputId": "c9cf60d0-93c3-470a-fdfd-4e6b53acfde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Survived         0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age            177\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             0\n",
              "Cabin          687\n",
              "Embarked         2\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bTzmKAxUO8h"
      },
      "outputs": [],
      "source": [
        "total = train_df.isnull().sum().sort_values(ascending=False)\n",
        "percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n",
        "percent_2 = (round(percent_1, 1)).sort_values(ascending=False) #rounds the values in percent_1 to one decimal place and sorts them in descending order.\n",
        "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
        "missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQtmPwj_MYOn"
      },
      "outputs": [],
      "source": [
        "# Check if there are any duplicates\n",
        "train_df.duplicated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laSAjcMXMcdT"
      },
      "outputs": [],
      "source": [
        "# Drop Duplicates\n",
        "train_df.drop_duplicates(inplace=True)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edusW6afMjbf"
      },
      "outputs": [],
      "source": [
        "# Check if there is any missing data\n",
        "train_df.isnull()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6-Ko0PETbMu"
      },
      "source": [
        "# METHOD-1: TO DROP NaN values\n",
        "# METHOD-2: TO fill NaN values with 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9Bckw6aTM2Z"
      },
      "outputs": [],
      "source": [
        "train_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pydt3HE4Mxfk"
      },
      "outputs": [],
      "source": [
        "# Fill missing data with 0\n",
        "train_df.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jmeMh2oQTOY"
      },
      "outputs": [],
      "source": [
        "train_df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGaNTRo5FFcn"
      },
      "source": [
        "**Spend some time with data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-lTSioeFFco"
      },
      "source": [
        "**Features from Dataset**\n",
        "- survival:    Survival [0 = No, 1 = Yes]\n",
        "- PassengerId: Unique Id of a passenger.\n",
        "- pclass:    Ticket class [ 1 = 1st, 2 = 2nd, 3 = 3rd]    \n",
        "- sex:    Sex     \n",
        "- Age:    Age in years     \n",
        "- sibsp:    # of siblings / spouses aboard the Titanic     \n",
        "- parch:    # of parents / children aboard the Titanic     \n",
        "- ticket:    Ticket number     \n",
        "- fare:    Passenger fare     \n",
        "- cabin:    Cabin number     \n",
        "- embarked:    Port of Embarkation - [C = Cherbourg, Q = Queenstown, S = Southampton]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2a6F7QxFFcp"
      },
      "outputs": [],
      "source": [
        "train_df.head(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOH7XHqkRxJH"
      },
      "outputs": [],
      "source": [
        "# Fill missing data with 0\n",
        "train_df.fillna(0,inplace=True)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Hb7W1-SFZA"
      },
      "outputs": [],
      "source": [
        "train_df.dropna(inplace=True)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEMjbJr4Hm_t"
      },
      "outputs": [],
      "source": [
        "train_df.duplicated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaGCQqjPUxuV"
      },
      "outputs": [],
      "source": [
        "train_df.head(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IqvllIjFFcp"
      },
      "source": [
        "+ Catogrical Values\n",
        "+ Different scale of numeric data\n",
        "+ Missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZNhpsZUFFcp"
      },
      "source": [
        "### Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mzT0sr9FFcp"
      },
      "outputs": [],
      "source": [
        "total = train_df.isnull().sum().sort_values(ascending=False)\n",
        "percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n",
        "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
        "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
        "missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JANmteYTFFcp"
      },
      "source": [
        "# EDA : Exploratory Data Analysis\n",
        "\n",
        "Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations[]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3703jrFFFcq"
      },
      "source": [
        "Age vs Sex vs Survival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95EZ4uipFFcq"
      },
      "outputs": [],
      "source": [
        "survived = 'survived'\n",
        "not_survived = 'not survived'\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
        "women = train_df[train_df['Sex']=='female']\n",
        "men = train_df[train_df['Sex']=='male']\n",
        "# Using histplot for histograms\n",
        "sns.histplot(women[women['Survived'] == 1].Age.dropna(), bins=18, label=survived, kde=False, ax=axes[0])\n",
        "sns.histplot(women[women['Survived'] == 0].Age.dropna(), bins=40, label=not_survived, kde=False, ax=axes[0])\n",
        "axes[0].legend()\n",
        "axes[0].set_title('Female')\n",
        "\n",
        "# Using histplot for histograms\n",
        "sns.histplot(men[men['Survived'] == 1].Age.dropna(), bins=18, label=survived, kde=False, ax=axes[1])\n",
        "sns.histplot(men[men['Survived'] == 0].Age.dropna(), bins=40, label=not_survived, kde=False, ax=axes[1])\n",
        "axes[1].legend()\n",
        "axes[1].set_title('Male')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNSA5K7qFFcq"
      },
      "source": [
        "Embarked vs Pclass vs Sex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoTdJPXBFFcq"
      },
      "outputs": [],
      "source": [
        "FacetGrid = sns.FacetGrid(train_df, row='Embarked', height=4.5, aspect=1.6)\n",
        "FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='dark:#1f77b4',  order=None, hue_order=None )\n",
        "FacetGrid.add_legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z6oRIz1FFcq"
      },
      "source": [
        "Pclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJZsSoJcFFcq"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='Pclass', y='Survived', data=train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn88qsvoFFcr"
      },
      "outputs": [],
      "source": [
        "grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=2.2, aspect=1.6)\n",
        "grid.map(plt.hist, 'Age', alpha=0.5, bins=20)\n",
        "\n",
        "#It plots the distribution of ages ('Age' column) for each combination of 'Survived' and 'Pclass'.\n",
        "#The alpha parameter controls the transparency of the bars, and bins specifies the number of bins in the histogram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF8pNDjBFFcr"
      },
      "source": [
        "SibSp and Parch (also some feature engg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Creating a new feature called 'relatives' in both the train_df and test_df DataFrames.\n",
        "\n",
        "- It calculates the total number of relatives for each passenger by summing the 'SibSp' (number of siblings/spouses aboard) and 'Parch' (number of parents/children aboard) columns."
      ],
      "metadata": {
        "id": "NVeSZ3qbEBnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RCTZX1nFFcr"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n",
        "    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n",
        "    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n",
        "    dataset['not_alone'] = dataset['not_alone'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zWlwT-jFFcr"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOYGdCCuFFcr"
      },
      "source": [
        "no of people alone or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_qo-rwYFFcr"
      },
      "outputs": [],
      "source": [
        "train_df['not_alone'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diBAmosjFFcs"
      },
      "outputs": [],
      "source": [
        "# Use catplot\n",
        "g = sns.catplot(x='relatives', y='Survived', data=train_df, aspect=2.5, kind='point')\n",
        "\n",
        "# You can customize the plot further if needed\n",
        "g.set_axis_labels('Number of Relatives', 'Survival Probability')\n",
        "g.set_xticklabels(rotation=45)  # Rotate x-axis labels for better visibility\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wp3YXBF4FFcs"
      },
      "outputs": [],
      "source": [
        "# df_train_drop = train_df.dropna()\n",
        "\n",
        "# sns.pairplot(df_train_drop, hue='Survived');\n",
        "sns.pairplot(train_df, hue='Survived');\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IJi4fOAFFcs"
      },
      "source": [
        "# Data preprocessing and Feature Engg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8S1GANkFFcs"
      },
      "source": [
        "drop passenger ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqkAjEaQFFcs"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop(['PassengerId'], axis=1)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uz41SlRFFcs"
      },
      "source": [
        "Fix cabin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HSKRR68FFcs"
      },
      "outputs": [],
      "source": [
        "train_df['Cabin'] #looks like the cabin number - lets convert to deck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yN_MEkXFFcs"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n",
        "\n",
        "    # Explicitly convert to string and then use apply with a lambda function to handle the extraction\n",
        "    dataset['Deck'] = dataset['Cabin'].astype(str).apply(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group() if re.compile(\"([a-zA-Z]+)\").search(x) else \"U\")\n",
        "\n",
        "    dataset['Deck'] = dataset['Deck'].map(deck)\n",
        "    dataset['Deck'] = dataset['Deck'].fillna(0)\n",
        "    dataset['Deck'] = dataset['Deck'].astype(int)\n",
        "#We take the 'Cabin' information and look for letters in it (like A, B, C, etc.).\n",
        "#This information is assumed to represent the deck where the passenger's cabin is located.\n",
        "#If we find a letter, we use it as the deck; otherwise, we set it to \"U\" (for unknown).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "ZzRzBNRWGTbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you can drop the 'Cabin' feature\n",
        "train_df = train_df.drop(['Cabin'], axis=1)\n",
        "test_df = test_df.drop(['Cabin'], axis=1)\n"
      ],
      "metadata": {
        "id": "Hhd33ZlUF0cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeWDYJ3dFFct"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNK-NxPTFFcx"
      },
      "source": [
        "**Fix** **Age**\n",
        "\n",
        "**Lets get the mean,std of age and create random ages in the range**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The process of generating random numbers and filling missing values in the 'Age' column is a technique known as imputation.\n",
        "- Imputation is used when there are missing values in a dataset, and it involves replacing those missing values with estimated or generated values.\n",
        "- In this specific case, the goal is to impute missing 'Age' values using random numbers within a certain range."
      ],
      "metadata": {
        "id": "Wy9oMYdrJTH3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD_Yk5VwFFcx"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    mean = train_df[\"Age\"].mean()\n",
        "    std = test_df[\"Age\"].std()\n",
        "    is_null = dataset[\"Age\"].isnull().sum()\n",
        "    # compute random numbers between the mean, std and is_null\n",
        "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
        "    # fill NaN values in Age column with random values generated\n",
        "    age_slice = dataset[\"Age\"].copy()\n",
        "    age_slice[np.isnan(age_slice)] = rand_age\n",
        "    dataset[\"Age\"] = age_slice\n",
        "    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\n",
        "train_df[\"Age\"].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu903p12FFcx"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCk_PEqFFcx"
      },
      "source": [
        "**Fix Embarked - only 3 missing so lets do most common**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NGiPvyjFFcx",
        "outputId": "387eafb8-b53e-4348-b059-05d714518448"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     891\n",
              "unique      4\n",
              "top         S\n",
              "freq      644\n",
              "Name: Embarked, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ],
      "source": [
        "train_df['Embarked'].describe()\n",
        "#top is the mode(most frequently occuring)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTrvNPFWFFcx"
      },
      "outputs": [],
      "source": [
        "common_value = 'S'\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPkIKR5WZOaS"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Embarked'].isnull()"
      ],
      "metadata": {
        "id": "oLwNybrCKMMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSr7K3xnFFcy"
      },
      "source": [
        "## Converting Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6_kQiLFFcy"
      },
      "source": [
        "Fare - convert to int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORBuJX7oFFcy"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Fare'] = dataset['Fare'].fillna(0)\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC3wHsF3FFcy"
      },
      "source": [
        "Name - Extract Titles from name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeHEN8EFcbuT"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', \\\n",
        "                                                'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "    # Remove titles from 'Name' column\n",
        "    dataset['Name'] = dataset['Name'].str.replace('Mr\\.', '').str.replace('Mrs\\.', '').str.replace('Miss\\.', '').str.replace('Master\\.', '')\n",
        "\n",
        "    dataset['Title'] = dataset['Title'].map(titles)\n",
        "    dataset['Title'] = dataset['Title'].fillna(0)\n",
        "\n",
        "# Display the modified train_df DataFrame\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc_pqih3FFcy"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLwsMfyLFFcy"
      },
      "source": [
        "Sex to numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWYc5wZjFFcy"
      },
      "outputs": [],
      "source": [
        "genders = {\"male\": 0, \"female\": 1}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Sex'] = dataset['Sex'].map(genders)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "SWQwE1t_MiGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iwQJsmmFFcz"
      },
      "source": [
        "Ticket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41tKG9LIFFcz",
        "outputId": "c7551569-df89-4ce0-f7ac-01ae5bb8c880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count        891\n",
              "unique       681\n",
              "top       347082\n",
              "freq           7\n",
              "Name: Ticket, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ],
      "source": [
        "train_df['Ticket'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYH2Q24fFFcz"
      },
      "source": [
        "too many unique values...lets drop it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wso-WwPPFFcz"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop(['Ticket'], axis=1)\n",
        "test_df = test_df.drop(['Ticket'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "Ss4Jsh52Mvag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzrfAMTwFFcz"
      },
      "source": [
        "Embarked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgXp_9LIFFcz"
      },
      "outputs": [],
      "source": [
        "ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset['Embarked'] = dataset['Embarked'].map(ports).fillna(0).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "aKLqFyKFM2Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X_qM6A4FFc0"
      },
      "source": [
        "## Creating Categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKcq69hJFFc0"
      },
      "source": [
        "Age - convert into buckets... be carefull that number of samples in each class should be kinda equal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caHn-AAlFFc0"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['Age'] = dataset['Age'].astype(int)\n",
        "    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n",
        "    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n",
        "    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n",
        "    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n",
        "    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n",
        "    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n",
        "    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n",
        "    dataset.loc[ dataset['Age'] > 66, 'Age'] = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg9tJX9CFFc0"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrBEN0-pFFc0"
      },
      "outputs": [],
      "source": [
        "train_df['Age'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPZ_NYhlFFc0"
      },
      "source": [
        "Fare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GThagKlnFFc1"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "\n",
        "for dataset in data:\n",
        "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
        "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
        "    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n",
        "    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n",
        "    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "VFLApFFfieys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj6aHypnFFc1"
      },
      "source": [
        "## Creating new features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLZGyJIiFFc1"
      },
      "source": [
        "Age time class since we saw a affect of both on surviving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfuevqXVFFc1"
      },
      "outputs": [],
      "source": [
        "data = [train_df, test_df]\n",
        "for dataset in data:\n",
        "    dataset['Age_Class']= dataset['Age']* dataset['Pclass']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "vzC722hoinZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQP_aLqfFFc1"
      },
      "source": [
        "Fare per person in family - is a indication of number of people and which class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFGmr8ghFFc1"
      },
      "outputs": [],
      "source": [
        "for dataset in data:\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n",
        "    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mEJEaRWFFc1"
      },
      "outputs": [],
      "source": [
        "train_df.head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}